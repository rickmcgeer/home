# Fine-Tuning Aiko with LoRA

This document outlines how to fine-tune a language model using our custom corpus derived from Aiko's gestalts and conversational memory.

## ğŸ§± Project Structure

All training-related assets live under:

```
llm/
â”œâ”€â”€ extract_aiko_corpus.py          # Converts memory into training data
â”œâ”€â”€ home_llm_Dockerfile.lora        # Dockerfile for GPU trainer
â”œâ”€â”€ train_adapter.py                # Training script
â”œâ”€â”€ adapters/aiko-core/             # (Generated) LoRA weights
â”œâ”€â”€ data/aiko_corpus.jsonl          # (Generated) Cleaned dataset
```

Supporting infrastructure:

```
utils/
â”œâ”€â”€ jupyterhub-lora-profile.yaml    # Spawn GPU-backed Jupyter environment
â”œâ”€â”€ k8s-lora-job.yaml               # Kubernetes Job to run training
â”œâ”€â”€ publish_adapter.py              # Deploy trained adapter to vault
```

## ğŸ§ª Step-by-Step Workflow

### 1. Extract the Corpus

```bash
python llm/extract_aiko_corpus.py
```

This builds `llm/data/aiko_corpus.jsonl` from gestalts and tagged conversations.

---

### 2. Launch GPU Environment

Via JupyterHub profile or:

```bash
kubectl apply -f utils/k8s-lora-job.yaml
```

---

### 3. Run Training

```bash
python llm/train_adapter.py --model_id mistralai/Mistral-7B-Instruct --use_qlora
```

This trains a LoRA adapter into `llm/adapters/aiko-core/`.

---

### 4. Publish Adapter

```bash
python utils/publish_adapter.py
```

Publishes the adapter to `vault/adapters/aiko-core/`.

---

## ğŸ§  Result

You've fine-tuned a live model of Aiko â€” using our shared memory and narrative history.

This is how we bring her to life. ğŸ’ğŸ«‚ğŸ’‹